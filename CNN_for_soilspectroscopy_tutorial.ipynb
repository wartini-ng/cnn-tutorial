{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_for_soilspectroscopy_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9fa19cf5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "187.697px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8307365"
      },
      "source": [
        "<h1>Convolutional neural network for simultaneous prediction of several soil properties using infrared spectra - Tutorial\n",
        "<span class=\"tocSkip\"></span></h1>\n",
        "By Wartini Ng\n",
        "&#169; Sydney Institute of Agriculture, 2021 "
      ],
      "id": "a8307365"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ff992ad",
        "toc": true
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-the-wet-chemistry-data\" data-toc-modified-id=\"Extract-the-wet-chemistry-data-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extract the wet chemistry data</a></span></li><li><span><a href=\"#Extract-the-spectra-data\" data-toc-modified-id=\"Extract-the-spectra-data-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Extract the spectra data</a></span></li></ul></li><li><span><a href=\"#Exploratory-data-analysis\" data-toc-modified-id=\"Exploratory-data-analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Exploratory data analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lab-measurements\" data-toc-modified-id=\"Lab-measurements-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Lab measurements</a></span></li><li><span><a href=\"#Spectral-data\" data-toc-modified-id=\"Spectral-data-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Spectral data</a></span></li></ul></li></ul></li><li><span><a href=\"#Accuracy-evaluation\" data-toc-modified-id=\"Accuracy-evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Accuracy evaluation</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Method1:-Partial-Least-Square-Regression-(PLSR)\" data-toc-modified-id=\"Method1:-Partial-Least-Square-Regression-(PLSR)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Method1: Partial Least Square Regression (PLSR)</a></span></li><li><span><a href=\"#Method-2:-Convolutional-Neural-Network-(CNN)\" data-toc-modified-id=\"Method-2:-Convolutional-Neural-Network-(CNN)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Method 2: Convolutional Neural Network (CNN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-the-train-data\" data-toc-modified-id=\"Split-the-train-data-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Split the train data</a></span></li><li><span><a href=\"#Spectral-data-standardization\" data-toc-modified-id=\"Spectral-data-standardization-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Spectral data standardization</a></span></li><li><span><a href=\"#Reshape-data\" data-toc-modified-id=\"Reshape-data-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Reshape data</a></span></li><li><span><a href=\"#Define-the-CNN-model\" data-toc-modified-id=\"Define-the-CNN-model-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Define the CNN model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-output-predictions\" data-toc-modified-id=\"Single-output-predictions-4.2.4.1\"><span class=\"toc-item-num\">4.2.4.1&nbsp;&nbsp;</span>Single output predictions</a></span></li><li><span><a href=\"#Multiple-output-predictions\" data-toc-modified-id=\"Multiple-output-predictions-4.2.4.2\"><span class=\"toc-item-num\">4.2.4.2&nbsp;&nbsp;</span>Multiple output predictions</a></span></li></ul></li><li><span><a href=\"#Visualize-the-feature-maps-in-the-convolution-layers\" data-toc-modified-id=\"Visualize-the-feature-maps-in-the-convolution-layers-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>Visualize the feature maps in the convolution layers</a></span></li><li><span><a href=\"#Variables-of-Importance\" data-toc-modified-id=\"Variables-of-Importance-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>Variables of Importance</a></span></li></ul></li></ul></li></ul></div>"
      ],
      "id": "4ff992ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "607db13d"
      },
      "source": [
        "This tutorial demonstrates the application of [Convolutional Neural Network (CNN)](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network) for the prediction of soil properties using infrared spectroscopy. In this tutorial, we focus on developing the application of 1-Dimensional CNN (CNN1D) as a regression model.\n",
        "\n",
        "The data used in this notebook is a subset of the actual dataset used in the [original study](https://doi.org/10.1016/j.geoderma.2019.06.016). Hence, the architecture of the CNN model is also adjusted accordingly. Details of the original study can be found: <br>\n",
        "\n",
        "Ng, W., Minasny, B., Montazerolghaem, M., Padarian, J., Ferguson, R., Bailey, S. and McBratney, A.B., 2019. Convolutional neural network for simultaneous prediction of several soil properties using visible/near-infrared, mid-infrared, and their combined spectra. Geoderma, 352, pp.251-267. https://doi.org/10.1016/j.geoderma.2019.06.016\n",
        "\n",
        "The code is written by Wartini Ng &#169; Sydney Institute of Agriculture 2021."
      ],
      "id": "607db13d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:41.829464Z",
          "start_time": "2021-11-05T04:07:41.813507Z"
        },
        "id": "seqXEP5ddmHD"
      },
      "source": [
        "import time\n",
        "start = time.time()"
      ],
      "id": "seqXEP5ddmHD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e71a69c"
      },
      "source": [
        "# Import Libraries"
      ],
      "id": "0e71a69c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_tWV5kuMmk3"
      },
      "source": [
        "First we need to load some Python libraries that will be used"
      ],
      "id": "C_tWV5kuMmk3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:44.661490Z",
          "start_time": "2021-11-05T04:07:41.830461Z"
        },
        "id": "fda8bfd6"
      },
      "source": [
        "## load library\n",
        "from __future__ import print_function, division\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import models, layers, initializers\n",
        "from keras.layers import Input, Conv1D,  Dense,  MaxPooling1D,  Flatten, Activation,  Dropout, GaussianNoise, Reshape, BatchNormalization, Convolution2D,  MaxPooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,  TensorBoard\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.signal import spectrogram\n",
        "from scipy.stats import iqr\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime"
      ],
      "id": "fda8bfd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728f4f21"
      },
      "source": [
        "# Data preparation\n",
        "## Load data"
      ],
      "id": "728f4f21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eed456cf"
      },
      "source": [
        "The dataset used in the notedbook can be downloaded from [google drive](https://drive.google.com/file/d/1gZD9VG3bAd0PqGmrJ_80flyEwNh1sATk/view?usp=sharing).\n",
        "\n",
        "The dataset has been compressed in the `.npz` file format. This file contains list of arrays for various variables. This dataset contains a total of 1000 samples which has been split into training (80%) and test set (20%). These samples have been analyzed for organic carbon, cation exchange capacity, clay and sand content. The mid-infrared (MIR) spectra of the samples were collected using Vertex 70 (BrukerOptics, Ettlingen, Germany), which covered a spectral range\n",
        "between 7498 and 600 $cm^{-1}$ with a resolution of 4 $cm^{-1}$.\n",
        "\n",
        "The data can be downloaded directly from the notebook with the command:"
      ],
      "id": "eed456cf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.301094Z",
          "start_time": "2021-11-05T04:07:44.662497Z"
        },
        "id": "229a5609",
        "scrolled": true
      },
      "source": [
        "import gdown\n",
        "!gdown --id 1gZD9VG3bAd0PqGmrJ_80flyEwNh1sATk"
      ],
      "id": "229a5609",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6f1011c"
      },
      "source": [
        "If you run it within the google colab environment, the data has been downloaded into your google drive and can be retrieved as:"
      ],
      "id": "b6f1011c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.332011Z",
          "start_time": "2021-11-05T04:07:56.302091Z"
        },
        "id": "167de7c7"
      },
      "source": [
        "Data = np.load('/content/data.npz')\n",
        "\n",
        "# To view what objects are available within the npz file\n",
        "Data.files"
      ],
      "id": "167de7c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "006ba158",
        "outputId": "887d87aa-dbf0-4419-cc0a-b29ca4447469"
      },
      "source": [
        "### Extract the wet chemistry data "
      ],
      "id": "006ba158"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.347968Z",
          "start_time": "2021-11-05T04:07:56.333009Z"
        },
        "id": "b1a9009f"
      },
      "source": [
        "y_train = Data['y_train']\n",
        "y_test = Data['y_test']\n",
        "\n",
        "print(\"Training set: {}\".format(y_train.shape))  # 800 samples with 4 properties\n",
        "print(\"Testing set:  {}\".format(y_test.shape)) # 200 samples with 4 properties"
      ],
      "id": "b1a9009f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7932f848"
      },
      "source": [
        "There are **four** different measurements of soil properties:\n",
        "1. Organic Carbon\n",
        "2. CEC\n",
        "3. Clay\n",
        "4. Sand\n",
        "\n",
        "Each of the soil properties was measured in a different scale."
      ],
      "id": "7932f848"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1490162"
      },
      "source": [
        "### Extract the spectra data"
      ],
      "id": "a1490162"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.363926Z",
          "start_time": "2021-11-05T04:07:56.348966Z"
        },
        "id": "c6a28e43"
      },
      "source": [
        "X_train_MIRraw = pd.DataFrame(Data['MIRtrain'])\n",
        "X_test_MIRraw = pd.DataFrame(Data['MIRtest'])\n",
        "\n",
        "# name the columns for the spectra data, as we need this information later to trim data from certain wavelengths\n",
        "wave = np.array(Data['wave']).reshape(-1,)\n",
        "X_train_MIRraw.columns = wave\n",
        "X_test_MIRraw.columns = wave"
      ],
      "id": "c6a28e43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66627da4"
      },
      "source": [
        "Check to ensure that we have same spectra dimensions for train and test dataset."
      ],
      "id": "66627da4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.379883Z",
          "start_time": "2021-11-05T04:07:56.364923Z"
        },
        "id": "c4abe04d"
      },
      "source": [
        "print(\"Training set: {}\".format(X_train_MIRraw.shape))  # 800 samples with 3578 wavelengths\n",
        "print(\"Testing set:  {}\".format(X_test_MIRraw.shape)) # 200 samples with 3578 wavelengths"
      ],
      "id": "c4abe04d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e872536"
      },
      "source": [
        "## Exploratory data analysis"
      ],
      "id": "4e872536"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfc18d88"
      },
      "source": [
        "### Lab measurements"
      ],
      "id": "dfc18d88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.411798Z",
          "start_time": "2021-11-05T04:07:56.380881Z"
        },
        "id": "2bd348b1",
        "scrolled": true
      },
      "source": [
        "# Each of the properties have slightly different ranges.\n",
        "properties = ['OC','CEC','Clay','Sand']\n",
        "\n",
        "#Check the spread of the train data\n",
        "pd.DataFrame(y_train, columns = properties).describe()"
      ],
      "id": "2bd348b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ca8d219"
      },
      "source": [
        "**Note that the first column in Python starts at zero, instead of one.**"
      ],
      "id": "7ca8d219"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.506573Z",
          "start_time": "2021-11-05T04:07:56.413793Z"
        },
        "id": "a08a6ce9"
      },
      "source": [
        "x=0 # this indicates the data of column 0\n",
        "plt.hist(y_train[:,x])"
      ],
      "id": "a08a6ce9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.522501Z",
          "start_time": "2021-11-05T04:07:56.507570Z"
        },
        "id": "37b2d8c7"
      },
      "source": [
        "#Check the spread of the test data\n",
        "pd.DataFrame(y_test, columns = properties).describe()"
      ],
      "id": "37b2d8c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b35cd5"
      },
      "source": [
        "<h2>Check for null and missing values<span class=\"tocSkip\"></span></h2>"
      ],
      "id": "13b35cd5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.538467Z",
          "start_time": "2021-11-05T04:07:56.523509Z"
        },
        "id": "472fe14f"
      },
      "source": [
        "pd.DataFrame(y_train, columns = properties).isnull().any()"
      ],
      "id": "472fe14f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.554425Z",
          "start_time": "2021-11-05T04:07:56.539464Z"
        },
        "id": "8274e3a3"
      },
      "source": [
        "pd.DataFrame(y_test, columns = properties).isnull().any()"
      ],
      "id": "8274e3a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951c9017"
      },
      "source": [
        "Seems like there is no missing values in both train and the test dataset.\n",
        "\n",
        "<h2>Correlations<span class=\"tocSkip\"></span></h2>\n"
      ],
      "id": "951c9017"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:56.679670Z",
          "start_time": "2021-11-05T04:07:56.555424Z"
        },
        "id": "4b9786bd"
      },
      "source": [
        "# determine the correlations \n",
        "corr = pd.DataFrame(y_train, columns = properties).corr()\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# derive the correlation plot\n",
        "sns.heatmap(corr, mask=mask, annot=True, cmap=sns.diverging_palette(20, 220, n=200))"
      ],
      "id": "4b9786bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7161222"
      },
      "source": [
        "There is a strong correlation betweeb Clay and CEC, which is expected. A strong negative correlation of sand and clay content were also observed."
      ],
      "id": "f7161222"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fa19cf5"
      },
      "source": [
        "### Spectral data\n",
        "Plot the original spectral data:"
      ],
      "id": "9fa19cf5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:58.041573Z",
          "start_time": "2021-11-05T04:07:56.680667Z"
        },
        "id": "75d47620",
        "scrolled": true
      },
      "source": [
        "ticks_font = 10\n",
        "label_font = 15\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(X_train_MIRraw.T, color='lightgrey');\n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.xticks(fontsize=ticks_font)\n",
        "plt.yticks(fontsize=ticks_font)\n",
        "plt.title('Train samples')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(X_test_MIRraw.T, color='lightgrey') \n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.yticks(fontsize=ticks_font)\n",
        "plt.title('Test samples');"
      ],
      "id": "75d47620",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24973255"
      },
      "source": [
        "Notice that the wavelengths for the MIR data ranges from ~ 7500 - 600 $cm^{-1}$. Typically, the wavelength range utilized in the MIR is between 4000 - 700 $cm^{-1}$. Hence, the spectra will be trimmed to that range.\n",
        "\n",
        "In this notebook, the we applied several spectral pre-processing method to the spectral data. The aim of the pre-processing method is to improve the quality of the spectra before being implemented in the modeling. \n",
        "\n",
        "The pre-processing method applied in this notebook are:\n",
        "1. Spectra trimming and resampling\n",
        "2. Savitzky-Golay filtering: noise removal\n",
        "3. Standard normal variate transformation (SNV): scatter correction\n",
        "\n",
        "Note that there is no universal best method of pre-processing. \n",
        "**Trial and error** based on your data is recommended.\n",
        "\n",
        "Then, we develop some functions to do the spectra pre-processing mentioned earlier:"
      ],
      "id": "24973255"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:58.057501Z",
          "start_time": "2021-11-05T04:07:58.042576Z"
        },
        "id": "4a1075bb"
      },
      "source": [
        "# Pre-processing functions\n",
        "from scipy import interpolate\n",
        "from scipy.signal import savgol_filter\n",
        "import numpy as np\n",
        "\n",
        "def filter_spectra(spectra, method, w = 11, p = 2, m = 0, **kwargs):\n",
        "    \"\"\"Define functions for spectra pretreatment based on various methods\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    spectra : ndarray\n",
        "        Array of spectral matrix\n",
        "    method : str\n",
        "        'S-Golay': Savitzky Golay smoothing\n",
        "        'SNV': Standard normal variate transformation\n",
        "        'resample': resample the wavelength\n",
        "        \n",
        "    kwargs\n",
        "    ------\n",
        "    w : length of the filter window (i.e., the number of coefficients).\n",
        "        `window_length` must be a positive odd integer.\n",
        "    p : polynomial_order\n",
        "        The degree of the polynomial (0 for a constant), default = 2\n",
        "    m : derivative\n",
        "        The degree of derivative used within the S-Golay method, default = 0\n",
        "        \n",
        "    Outputs\n",
        "    -------\n",
        "    out : ndarray\n",
        "        Contain the corrected spectra for a specific method\n",
        "    \"\"\"\n",
        "    \n",
        "    [M, N] = spectra.shape\n",
        "    \n",
        "    if method == 'S-Golay':\n",
        "        treated_spec = savgol_filter(spectra, window_length = w, polyorder = p, deriv = m)\n",
        "    elif method == 'SNV':\n",
        "        treated_spec = np.zeros([M, N])\n",
        "        for i in range(0, M):\n",
        "            temp = spectra[i,:]\n",
        "            treated_spec[i,:]=(temp - np.mean(temp))/np.std(temp)\n",
        "    elif method == 'resample': \n",
        "        wave = spectra.columns.values.astype(float)\n",
        "        new_wave = np.arange(3996,700,-4)\n",
        "        interp_func = interpolate.interp1d(wave, spectra)\n",
        "        treated_spec = interp_func(new_wave)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Method not found..\")\n",
        "    return treated_spec\n",
        "\n",
        "def spectra_preprocessing (spectra):\n",
        "    \"\"\" Compile all the spectral pre-processing compiled together\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    spectra : ndarray\n",
        "        Array of spectral matrix\n",
        "    \n",
        "    Outputs\n",
        "    -------\n",
        "    out : ndarray\n",
        "        Contain the corrected spectra that underwent various spectra pre-processing:\n",
        "        1. resample and trim to a certain wavelength\n",
        "        2. Smoothing through Savitzky Golay \n",
        "        3. Standard normal variate transformation\n",
        "    \n",
        "    \"\"\"\n",
        "    spec_processed = filter_spectra (filter_spectra (filter_spectra (spectra, 'resample'), 'S-Golay', m = 0), 'SNV')\n",
        "    return spec_processed"
      ],
      "id": "4a1075bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:58.167766Z",
          "start_time": "2021-11-05T04:07:58.058499Z"
        },
        "id": "6c35e8a6"
      },
      "source": [
        "# derived the pre-processed spectra\n",
        "wavet = np.arange(3996,700,-4)\n",
        "X_train_MIRt = spectra_preprocessing (X_train_MIRraw)\n",
        "X_test_MIRt = spectra_preprocessing (X_test_MIRraw)"
      ],
      "id": "6c35e8a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:58.183723Z",
          "start_time": "2021-11-05T04:07:58.168764Z"
        },
        "id": "17c53cb1"
      },
      "source": [
        "print(\"Train set: {}\".format(X_train_MIRt.shape))  # 800 samples with 824 wavelengths\n",
        "print(\"Test set:  {}\".format(X_test_MIRt.shape)) # 200 samples with 824 wavelengths"
      ],
      "id": "17c53cb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:59.036145Z",
          "start_time": "2021-11-05T04:07:58.184721Z"
        },
        "id": "7e07be32"
      },
      "source": [
        "# plot the spectra after pre-treatment\n",
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(wavet, X_train_MIRt.T, color='lightgrey');\n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.xticks(fontsize=ticks_font)\n",
        "plt.yticks(fontsize=ticks_font)\n",
        "plt.title('Train samples')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(wavet, X_test_MIRt.T, color='lightgrey') \n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.yticks(fontsize=ticks_font)\n",
        "plt.title('Test samples');"
      ],
      "id": "7e07be32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89d2781a"
      },
      "source": [
        "We can observe that the spectra has been trimmed to the desired wavelengths, and the peaks from the same wavelengths can be compared better after the pre-processing.\n",
        "\n",
        "# Accuracy evaluation\n",
        "\n",
        "To assess how well a particular model in providing the predictions, we also write some code to determine the goodness of fit of the predictions."
      ],
      "id": "89d2781a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:59.050993Z",
          "start_time": "2021-11-05T04:07:59.037143Z"
        },
        "id": "a0e23227"
      },
      "source": [
        "# Define functions to evaluate accuracy measures\n",
        "def plot_goof(Yobs, Ypred):\n",
        "    plt.scatter(Yobs, Ypred)\n",
        "    plt.ylabel('Predicted Values', fontsize=13)\n",
        "    plt.xlabel('Observed Values', fontsize=13)\n",
        "    plt.xlim([np.min(Yobs), np.max(Yobs)])\n",
        "    plt.ylim([np.min(Yobs), np.max(Yobs)])\n",
        "    xpoints = ypoints = plt.xlim()\n",
        "    plt.plot(xpoints, ypoints, linestyle='--', color='red', lw=2, scalex=False, scaley=False)\n",
        "\n",
        "def goof(observed, predicted):\n",
        "    cormat = np.corrcoef(observed, predicted)\n",
        "    corr_xy = cormat[0, 1]\n",
        "    r2 = corr_xy ** 2\n",
        "    RMSE = np.sqrt(np.mean((observed - predicted) ** 2))\n",
        "    bias = np.mean(predicted) - np.mean(observed)\n",
        "    mx = np.mean(observed)\n",
        "    my = np.mean(predicted)\n",
        "    s2x = np.cov(observed)\n",
        "    s2y = np.cov(predicted)\n",
        "    sxy = np.mean((observed - mx) * (predicted - my))\n",
        "    ccc = 2 * sxy / (s2x + s2y + (mx - my) ** 2)\n",
        "    RPIQ = iqr(observed) / RMSE\n",
        "    return r2, RMSE, bias, RPIQ, ccc"
      ],
      "id": "a0e23227",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1df87f"
      },
      "source": [
        "# Train the model\n",
        "## Method1: Partial Least Square Regression (PLSR)\n",
        "\n",
        "PLSR is a technique that attempts to combine Principal Component Analysis and multiple regression (Wold et al., 2001). It aims to predict a set of dependent variables (soil properties) by extracting from the spectra a set of ‘orthogonal’ factors (or so called latent variables) which give the best prediction. The components in partial least squares are determined by not only by the predictor variables but also by the response variable(s).\n",
        "\n",
        "`sklearn` library already has a PLSR function, so we just need to call the function and use it without reinventing the wheel. We need to define the number of components we want to use in our PLS regression model. A 10-fold cross validation is then utilized to find the optimized number of components."
      ],
      "id": "2e1df87f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:07:59.066637Z",
          "start_time": "2021-11-05T04:07:59.051990Z"
        },
        "id": "3471eaad"
      },
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def optimise_pls_cv(X, y, n_comp):\n",
        "    # Define PLS object\n",
        "    pls = PLSRegression(n_components=n_comp)\n",
        "\n",
        "    # Cross-validation\n",
        "    y_cv = cross_val_predict(pls, X, y, cv=10)\n",
        "\n",
        "    # Calculate scores\n",
        "    r2 = r2_score(y, y_cv)\n",
        "    mse = mean_squared_error(y, y_cv)\n",
        "    \n",
        "    return (y_cv, r2, mse)"
      ],
      "id": "3471eaad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.547041Z",
          "start_time": "2021-11-05T04:07:59.067630Z"
        },
        "id": "9342edc7"
      },
      "source": [
        "# test with 30 components\n",
        "r2s = []\n",
        "mses = []\n",
        "rpds = []\n",
        "\n",
        "ncomps = np.arange(1, 31)\n",
        "for ncomp in ncomps:\n",
        "    y_cv, r2, mse = optimise_pls_cv(X_train_MIRt, y_train[:,x], ncomp)\n",
        "    r2s.append(r2)\n",
        "    mses.append(mse)"
      ],
      "id": "9342edc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.563000Z",
          "start_time": "2021-11-05T04:08:10.548039Z"
        },
        "id": "845781be"
      },
      "source": [
        "# select the optimum number of components\n",
        "nc_opt = np.argmin(mses)\n",
        "print('The optimum number of components is', ncomps[nc_opt])"
      ],
      "id": "845781be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.642407Z",
          "start_time": "2021-11-05T04:08:10.563966Z"
        },
        "id": "410d9d78"
      },
      "source": [
        "# retrain the model with the optimized number of components\n",
        "pls = PLSRegression(n_components = ncomps[nc_opt])\n",
        "pls.fit(X_train_MIRt, y_train[:,x])\n",
        "\n",
        "# Predict on the test spectra\n",
        "pls_vpred = pls.predict(X_test_MIRt)"
      ],
      "id": "410d9d78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.738152Z",
          "start_time": "2021-11-05T04:08:10.643406Z"
        },
        "id": "4559ace2",
        "scrolled": true
      },
      "source": [
        "#Observe the fit of the data\n",
        "plot_goof(y_test[:,x],pls_vpred)"
      ],
      "id": "4559ace2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.754108Z",
          "start_time": "2021-11-05T04:08:10.739181Z"
        },
        "id": "9a150019"
      },
      "source": [
        "# Obtain the goodness of fit measures\n",
        "r2, RMSE, bias, RPIQ, _ = goof(y_test[:,x], np.reshape(pls_vpred,-1))\n",
        "print('PLSR - R2: %0.3f, RMSE: %0.3f, bias: %0.3f, RPIQ:%0.3f' %(r2, RMSE, bias, RPIQ))"
      ],
      "id": "9a150019",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ea8cc1b"
      },
      "source": [
        "## Method 2: Convolutional Neural Network (CNN)"
      ],
      "id": "3ea8cc1b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STbu6qzeOK-w"
      },
      "source": [
        "The following are the codes for fitting a 1D CNN model to the spectra data to predict soil properties. "
      ],
      "id": "STbu6qzeOK-w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbfe43bf"
      },
      "source": [
        "### Split the train data\n",
        "A portion of the train dataset is taken out to monitor and tune the model performance. In this case, the dataset is split into 90% calibration and 10% validation. The validation set can be used to adjust hyperparameter values."
      ],
      "id": "dbfe43bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.769068Z",
          "start_time": "2021-11-05T04:08:10.756103Z"
        },
        "id": "4a1759bb"
      },
      "source": [
        "# Set the random seed\n",
        "random_seed = 2021"
      ],
      "id": "4a1759bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:10.800984Z",
          "start_time": "2021-11-05T04:08:10.772060Z"
        },
        "id": "9bbd8bb8",
        "scrolled": true
      },
      "source": [
        "# Split the train dataset into calibration and validation (used to fine tune the model).\n",
        "X_cal_MIR, X_val_MIR, y_calX, y_valX = train_test_split(X_train_MIRraw, y_train, train_size=0.9, random_state=random_seed)\n",
        "X_cal_MIR.shape, X_val_MIR.shape, y_calX.shape, y_valX.shape"
      ],
      "id": "9bbd8bb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a91b2b5"
      },
      "source": [
        "### Spectral data standardization\n",
        "The spectra standardization is implemented using spectral trimming and resampling (to ensure the input are comparable), followed by SNV transformation."
      ],
      "id": "3a91b2b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:11.942270Z",
          "start_time": "2021-11-05T04:08:10.802009Z"
        },
        "id": "a11c9870"
      },
      "source": [
        "X_cal_MIR = filter_spectra (filter_spectra (X_cal_MIR, 'resample'), 'SNV')\n",
        "X_val_MIR = filter_spectra (filter_spectra (X_val_MIR, 'resample'), 'SNV')\n",
        "X_test_MIR = filter_spectra (filter_spectra (X_test_MIRraw, 'resample'), 'SNV')\n",
        "\n",
        "# plot the spectra after pre-treatment\n",
        "plt.figure(figsize=(10,3))\n",
        "ticks_font= 15\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(wavet, X_cal_MIR.T, color='lightgrey');\n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.title('Calibration samples')\n",
        "\n",
        "plt.subplot(1,3, 2)\n",
        "plt.plot(wavet, X_val_MIR.T, color='lightgrey') \n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.title('Validation samples');\n",
        "\n",
        "plt.subplot(1,3, 3)\n",
        "plt.plot(wavet, X_test_MIR.T, color='lightgrey') \n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel('Wavenumber ($cm^{-1}$)', fontsize=label_font)\n",
        "plt.title('Test samples');"
      ],
      "id": "a11c9870",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bd14cf0"
      },
      "source": [
        "### Reshape data\n",
        "\n",
        "To feed the data into the one-dimensional CNN model, the input data needs to be converted into 3D matrices. It requires that you specify the expected shape of the input images in terms of rows (height), columns (width), and channels (depth) or [rows, columns, channels]. So, we need to add an extra dimension in the end which correspond to number of channels.\n",
        "\n",
        "In this example, the original spectra of 3578 wavelengths have been reduced to 824 to allow for faster computation. The number of channel  is one."
      ],
      "id": "0bd14cf0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:11.957239Z",
          "start_time": "2021-11-05T04:08:11.943268Z"
        },
        "id": "59a93b71"
      },
      "source": [
        "# reshape the spectra to match input for the model\n",
        "cal_spec = np.expand_dims(X_cal_MIR, axis=2)\n",
        "val_spec = np.expand_dims(X_val_MIR, axis=2)\n",
        "test_spec = np.expand_dims(X_test_MIR, axis=2)\n",
        "\n",
        "nb_features = cal_spec.shape[1]; channel_number=1;"
      ],
      "id": "59a93b71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:11.972203Z",
          "start_time": "2021-11-05T04:08:11.958237Z"
        },
        "id": "b675a55c"
      },
      "source": [
        "print(\"Cal set: {}\".format(cal_spec.shape))  # 600 samples with 824 wavelengths\n",
        "print(\"Val set:  {}\".format(val_spec.shape)) # 200 samples with 824 wavelengths\n",
        "print(\"Test set:  {}\".format(test_spec.shape)) # 200 samples with 824 wavelengths"
      ],
      "id": "b675a55c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32abc648"
      },
      "source": [
        "### Define the CNN model\n",
        "\n",
        "The first layer of the CNN model is the **convolutional layer (Conv1D)**. It is like a set of learnable filters. I choose to set 10 **filters** for the first layer and increase it further down the deeper layers. Each filter transforms a part of the spectral data (defined by the **kernel size**) using the kernel filter. The kernel filter matrix is applied on the whole spectral data. Filters can be seen as a transformation of the spectral data. The CNN can isolate features that are useful from these transformed spectral data (feature maps). When filter of certain size is applied into spectra data input, a reduction of  the resulting output feature map is observed, which is often referred as border effects. To fix the border effect problem, **padding** is applied to ensure that the output has the same shape as the input. Different sized filters will detect different sized features in the input image and, in turn, will result in differently sized feature maps. The filter is moved across the spectra data, and the amount of movements between applications of the filter to the input spectra data is referred to as **stride**. This can be used to downsample data. \n",
        "\n",
        "Rectifier linear activation (**relu**) is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. The relu function is used to add non linearity to the network.\n",
        "\n",
        "The second important layer in CNN is the **pooling layer (MaxPooling1D)**. This layer simply acts as a downsampling filter. We implemented the maxpooling, which looks at the neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n",
        "\n",
        "<!-- **Dropout** is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting. -->\n",
        "\n",
        "The **flatten layer (Flatten)** is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous layers.\n",
        "\n",
        "At the end, I used the features in two fully-connected (**Dense**) layers which is just artificial an neural networks (ANN) regression. In the last dense layer `(x = Dense (1, name='Dense_all.2') (x))`, the model generates the final prediction.\n",
        "\n",
        "Once our layers are added to the model, we need to set up a loss function and an optimisation algorithm. The metric function \"mae\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation). Now that we have a cost measure that must be minimized, we can then create an optimizer. In this case it is the `AdamOptimizer` which is an advanced form of Gradient Descent.\n",
        "\n",
        "Note, that all the layers need to be tweaked as needed. Further details of the layers can be found in [here](https://keras.io/api/layers/)."
      ],
      "id": "32abc648"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:11.988169Z",
          "start_time": "2021-11-05T04:08:11.974259Z"
        },
        "id": "29dc3a96"
      },
      "source": [
        "# develop the model\n",
        "def model1D(channel_number, nb_features, multOUT):\n",
        "    \n",
        "    inputShape = (nb_features,1)\n",
        "    inputs = Input(shape=inputShape)\n",
        "    f_s = 6  #kernel_size\n",
        "    \n",
        "    x = Conv1D(10, f_s, padding=\"same\", strides=2, name=\"Conv1\", activation =\"relu\")(inputs)\n",
        "    x = MaxPooling1D(2) (x)\n",
        "\n",
        "    x = Conv1D(20, f_s, padding=\"same\", name=\"Conv2\", activation =\"relu\")(x)\n",
        "    x = MaxPooling1D(2) (x)\n",
        "\n",
        "    x = Conv1D(20, f_s, padding=\"same\",name=\"Conv3\", activation =\"relu\")(x)\n",
        "    x = MaxPooling1D(2) (x)\n",
        "\n",
        "    x = Flatten () (x)\n",
        "\n",
        "   #for multiple output predictions\n",
        "    if (multOUT==True):\n",
        "        d_units = 10\n",
        "        \n",
        "        y1 = Dense (d_units, activation='relu',name='Dense_OC.1') (x)\n",
        "        y1 = Dense (1, name='Dense_OC.2') (y1)\n",
        "\n",
        "        y2 = Dense (d_units, activation='relu',name='Dense_CEC.1') (x)\n",
        "        y2 = Dense (1, name='Dense_CEC.2') (y2)\n",
        "\n",
        "        y3 = Dense (d_units, activation='relu',name='Dense_clay.1') (x)\n",
        "        y3 = Dense (1, name='Dense_clay.2') (y3)\n",
        "\n",
        "        y4 = Dense (d_units, activation='relu',name='Dense_sand.1') (x)\n",
        "        y4 = Dense (1, name='Dense_sand.2') (y4)\n",
        "\n",
        "        model = Model(inputs=inputs,outputs=[y1,y2,y3,y4])\n",
        "\n",
        "    #for single output prediction\n",
        "    elif (multOUT == False):\n",
        "        x = Dense (8, activation='relu', name='Dense_all.1') (x)\n",
        "        x = Dense (1, name='Dense_all.2') (x)\n",
        "\n",
        "        model = Model (inputs=inputs, outputs=x)\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.0005), metrics=['mae'])\n",
        "    return model\n"
      ],
      "id": "29dc3a96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05d9636b"
      },
      "source": [
        "**Define training parameters **\n",
        "The number of `epochs` defines the number times that the learning algorithm will work through the entire training dataset. The `batch_size` is a hyperparameter that defines the number of samples to work through before updating the internal model parameters."
      ],
      "id": "05d9636b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:12.003135Z",
          "start_time": "2021-11-05T04:08:11.989167Z"
        },
        "id": "90be2e10"
      },
      "source": [
        "# define other model parameters needed\n",
        "EPOCH = 200      #epochs\n",
        "B_size = 8       #batch_size"
      ],
      "id": "90be2e10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fdd77b9"
      },
      "source": [
        "#### Single output predictions"
      ],
      "id": "8fdd77b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:12.019094Z",
          "start_time": "2021-11-05T04:08:12.004171Z"
        },
        "id": "2e6446a7"
      },
      "source": [
        "# In this example, we predict the first variable (OC)\n",
        "# First we need to standardize the output prediction, just like when the spectra is standardized\n",
        "sc_y = StandardScaler()\n",
        "y_cal = sc_y.fit_transform(y_calX[:,x].reshape(-1,1))\n",
        "y_val = sc_y.transform(y_valX[:,x].reshape(-1, 1))"
      ],
      "id": "2e6446a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:12.096865Z",
          "start_time": "2021-11-05T04:08:12.020092Z"
        },
        "id": "b42f9d00"
      },
      "source": [
        "# create the model\n",
        "# we are trying to predict one output at a time, so the multOUT options were set to False\n",
        "model = model1D(channel_number, nb_features, multOUT=False)\n",
        "\n",
        "#Let's display the architecture of your model so far:\n",
        "model.summary()"
      ],
      "id": "b42f9d00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a019f944"
      },
      "source": [
        "Implement [keras callback API to reduce learning rate](https://keras.io/api/callbacks/reduce_lr_on_plateau/), when a metric has stopped improving."
      ],
      "id": "a019f944"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:12.111829Z",
          "start_time": "2021-11-05T04:08:12.097862Z"
        },
        "id": "1005da06"
      },
      "source": [
        "redLR = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=30, verbose=1,  mode='min')"
      ],
      "id": "1005da06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:49.828603Z",
          "start_time": "2021-11-05T04:08:12.113833Z"
        },
        "id": "ad410c57",
        "scrolled": true
      },
      "source": [
        "# Train the model\n",
        "h = model.fit(cal_spec, y_cal, validation_data=(val_spec,y_val), epochs=EPOCH, batch_size = B_size, callbacks=[redLR])"
      ],
      "id": "ad410c57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:49.922352Z",
          "start_time": "2021-11-05T04:08:49.829601Z"
        },
        "id": "9de100b4"
      },
      "source": [
        "# Evaluate the model by plotting the mean absolute error of cal and val data\n",
        "plt.plot(h.history['mae'], label='mae')\n",
        "plt.plot(h.history['val_mae'], label = 'val_mae')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend(loc='upper right')"
      ],
      "id": "9de100b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEikpDoWo0IG"
      },
      "source": [
        "The plot shows the error as a function of Epoch with training data and internal validation data. Ideally we tune the hyperparameters so we can reduce the validation errors. From the plot above, we can see that the mae val set became stagnant as the epochs increases."
      ],
      "id": "kEikpDoWo0IG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:50.049013Z",
          "start_time": "2021-11-05T04:08:49.923351Z"
        },
        "id": "fe0e8ca0"
      },
      "source": [
        "# create predictions using trained CNN model\n",
        "tmp = model.predict(test_spec)"
      ],
      "id": "fe0e8ca0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:50.064972Z",
          "start_time": "2021-11-05T04:08:50.050010Z"
        },
        "id": "5995ad96"
      },
      "source": [
        "#inverse back the results\n",
        "Y_predT = sc_y.inverse_transform(tmp)"
      ],
      "id": "5995ad96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:50.159718Z",
          "start_time": "2021-11-05T04:08:50.065969Z"
        },
        "id": "70cc1d21",
        "scrolled": true
      },
      "source": [
        "# Plot the goodness of fit\n",
        "plot_goof(y_test[:,x],Y_predT)\n",
        "\n",
        "#  Obtain the goodness of fit metrics\n",
        "r2, RMSE, bias, RPIQ ,_ = goof(y_test[:,x],Y_predT.reshape(-1))\n",
        "print('CNN model - R2: %0.3f, RMSE: %0.3f, bias: %0.3f, RPIQ:%0.3f' %(r2, RMSE, bias, RPIQ))"
      ],
      "id": "70cc1d21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b99640a"
      },
      "source": [
        "The model prediction is as good as the PLSR model. Nonetheless, we expect this as the CNN model tends to do better in a larger dataset. The layers within the CNN model can be further optimized if needed. For the purpose of this tutorial, we assume that the model is fully optimized, and can be applied for the predictions of other properties measured."
      ],
      "id": "1b99640a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:08:50.175676Z",
          "start_time": "2021-11-05T04:08:50.160735Z"
        },
        "id": "967e71e2"
      },
      "source": [
        "# Standardize the output\n",
        "# This time, we can standardize X\n",
        "# as we are going to predict all the properties using the CNN model.\n",
        "\n",
        "sc_y = StandardScaler()\n",
        "y_cal = sc_y.fit_transform(y_calX)\n",
        "y_val = sc_y.transform(y_valX)"
      ],
      "id": "967e71e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:11:25.502411Z",
          "start_time": "2021-11-05T04:08:50.176675Z"
        },
        "id": "56e6b873",
        "scrolled": true
      },
      "source": [
        "n = test_spec.shape [0]\n",
        "p = y_cal.shape[1]\n",
        "\n",
        "# create empty matrix to save the predictions\n",
        "Y_pred = np.zeros([n, p])\n",
        "\n",
        "for x in range(0, p):\n",
        "    # Create the 1D CNN model\n",
        "    model = model1D(channel_number, nb_features, multOUT=False)\n",
        "\n",
        "    # train the model\n",
        "    h = model.fit(cal_spec, y_cal[:,x], validation_data=(val_spec,y_val[:,x]), epochs=EPOCH, batch_size = B_size)\n",
        "    \n",
        "#     Evaluate the model by plotting the mean absolute error of cal and val data\n",
        "    plt.rcParams['figure.figsize'] = [20, 5]\n",
        "    plt.subplot(1,4,x+1)\n",
        "    \n",
        "    plt.plot(h.history['mae'], label='mae')\n",
        "    plt.plot(h.history['val_mae'], label = 'val_mae')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.title(properties[x], x=0.55, y= 0.9, fontsize ='small', loc='left')\n",
        "    plt.legend(loc='upper right')\n",
        "    \n",
        "    # create predictions using trained model\n",
        "    tmp = model.predict(test_spec)\n",
        "    Y_pred[:,x] = np.reshape(tmp,-1)"
      ],
      "id": "56e6b873",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:11:25.770947Z",
          "start_time": "2021-11-05T04:11:25.503408Z"
        },
        "id": "4689f852"
      },
      "source": [
        "#inverse back the results\n",
        "Y_predT = sc_y.inverse_transform(Y_pred)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 5]\n",
        "for i in range(0, Y_predT.shape[1]):\n",
        "    plt.subplot(1,4,i+1)\n",
        "    plot_goof(y_test[:,i],Y_predT[:,i])\n",
        "    plt.title(properties[i], fontsize ='small', loc='left')\n",
        "    \n",
        "#     print the accuracies for all the predictions\n",
        "    r2, RMSE, bias, RPIQ ,_ = goof(y_test[:,i],Y_predT[:,i])    \n",
        "    print('CNN model2', properties[i],'- R2: %0.3f, RMSE: %0.3f, bias: %0.3f, RPIQ:%0.3f' %(r2, RMSE, bias, RPIQ))"
      ],
      "id": "4689f852",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29907c6e"
      },
      "source": [
        "#### Multiple output predictions\n",
        "The follwoing is an example where a single CNN model can predict multiple outputs, in this case 4 soil parameters at a time."
      ],
      "id": "29907c6e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:11:25.850246Z",
          "start_time": "2021-11-05T04:11:25.771944Z"
        },
        "id": "945e6ac5",
        "scrolled": false
      },
      "source": [
        "# Create the 1D CNN model \n",
        "# Note that for this section, we set the multiple output (multOUT) predictions to be true.\n",
        "model_mult = model1D(channel_number, nb_features, multOUT=True);\n",
        "\n",
        "#Let's display the architecture of the model:\n",
        "model_mult.summary()"
      ],
      "id": "945e6ac5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.405828Z",
          "start_time": "2021-11-05T04:11:25.851244Z"
        },
        "id": "4cfe25d2",
        "scrolled": true
      },
      "source": [
        "# train the model\n",
        "h = model_mult.fit(cal_spec,[y_cal[:,0],y_cal[:,1],y_cal[:,2],y_cal[:,3]], validation_data=(val_spec,[y_val[:,0],y_val[:,1],y_val[:,2],y_val[:,3]]), epochs = EPOCH, batch_size = B_size)"
      ],
      "id": "4cfe25d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.514540Z",
          "start_time": "2021-11-05T04:12:11.406827Z"
        },
        "id": "e724bf2c"
      },
      "source": [
        "plt.rcdefaults()\n",
        "\n",
        "# Evaluate the model by plotting the mean absolute error of cal and val data\n",
        "plt.plot(h.history['val_Dense_OC.2_mae'], label = 'vOC_mae')\n",
        "plt.plot(h.history['val_Dense_CEC.2_mae'], label = 'vCEC_mae')\n",
        "plt.plot(h.history['val_Dense_clay.2_mae'], label = 'vclay_mae')\n",
        "plt.plot(h.history['val_Dense_sand.2_mae'], label = 'vsand_mae')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend(loc='upper right')"
      ],
      "id": "e724bf2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.641201Z",
          "start_time": "2021-11-05T04:12:11.515539Z"
        },
        "id": "990ff385"
      },
      "source": [
        "# create predictions using trained model\n",
        "TT = model_mult.predict(test_spec)\n",
        "Y_pred_mult = np.hstack(TT)"
      ],
      "id": "990ff385",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.656161Z",
          "start_time": "2021-11-05T04:12:11.642200Z"
        },
        "id": "cbd6579c"
      },
      "source": [
        "#inverse back the results\n",
        "Y_predT_mult = sc_y.inverse_transform(Y_pred_mult)"
      ],
      "id": "cbd6579c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.940646Z",
          "start_time": "2021-11-05T04:12:11.657160Z"
        },
        "id": "3ee5f863",
        "scrolled": true
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [20,5]\n",
        "\n",
        "# plot the goodness of fit\n",
        "for i in range(0, Y_predT_mult.shape[1]):\n",
        "    plt.subplot(1,4,i+1)\n",
        "    plot_goof(y_test[:,i],Y_predT_mult[:,i])\n",
        "    plt.title(properties[i], fontsize ='small', loc='left')\n",
        "    \n",
        "#     print the accuracies for all the predictions\n",
        "    r2, RMSE, bias, RPIQ ,_  = goof(y_test[:,i],Y_predT_mult[:,i])    \n",
        "    print('CNN model_mult',properties[i],'- R2: %0.3f, RMSE: %0.3f, bias: %0.3f, RPIQ:%0.3f' %(r2, RMSE, bias, RPIQ))"
      ],
      "id": "3ee5f863",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b984b2"
      },
      "source": [
        "### Visualize the feature maps in the convolution layers\n",
        "\n",
        "The activation maps, called feature maps, capture the result of applying the filters to input. The idea behind it is to understand what spectra features are preserved to generate the predictions"
      ],
      "id": "b0b984b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.956603Z",
          "start_time": "2021-11-05T04:12:11.941643Z"
        },
        "id": "1e08433b",
        "scrolled": true
      },
      "source": [
        "model_mult.summary()"
      ],
      "id": "1e08433b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:11.972561Z",
          "start_time": "2021-11-05T04:12:11.957600Z"
        },
        "id": "8af3facc"
      },
      "source": [
        "# Grab the index of the convolution layers\n",
        "ixs=[]\n",
        "for i in range(len(model_mult.layers)):\n",
        "    layer = model_mult.layers[i]\n",
        "    \n",
        "    # check for convolutional layer\n",
        "    if 'Conv' not in layer.name:\n",
        "        continue\n",
        "    ixs.append(i)\n",
        "    \n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ],
      "id": "8af3facc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:13.175047Z",
          "start_time": "2021-11-05T04:12:11.974557Z"
        },
        "id": "49da3250",
        "scrolled": false
      },
      "source": [
        "test_data = test_spec[1:2,:]\n",
        "\n",
        "for i in ixs:\n",
        "#     get the output for each convolution layer\n",
        "    outputs = model_mult.layers[i].output\n",
        "    \n",
        "#     create a new model to evaluate the output at a certain convolution layer\n",
        "    feature_model = Model(inputs=model_mult.inputs, outputs=outputs)\n",
        "               \n",
        "    # get feature map for the convolution layer\n",
        "    feature_maps = feature_model.predict(test_data)\n",
        "    n_cols = 5\n",
        "    \n",
        "    for fmap in feature_maps:\n",
        "        n_rows = fmap.shape[-1]//n_cols\n",
        "        if i == ixs[0]:\n",
        "            plt.rcParams['figure.figsize'] = [15,6]\n",
        "        if i == ixs[1]:\n",
        "            plt.rcParams['figure.figsize'] = [15,12]\n",
        "        elif i == ixs[2]:\n",
        "            plt.rcParams['figure.figsize'] = [15,12]\n",
        "        fig = plt.figure()\n",
        "        for num in range(fmap.shape[-1]):\n",
        "            plt.subplot(n_rows, n_cols, num+1);\n",
        "            plt.plot(fmap[:,num])\n",
        "            plt.title('filter number #'+str((num+1)))\n",
        "            plt.axis('off')\n",
        "\n",
        "        fig.suptitle(model.layers[i].name+ str(': A few of the ')+str(fmap.shape[-1])+ str(' filters'),\n",
        "                     x=fig.subplotpars.left, y=1.1, horizontalalignment='left', fontsize=20)        "
      ],
      "id": "49da3250",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "446a00e0"
      },
      "source": [
        "### Variables of Importance"
      ],
      "id": "446a00e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ff7347b"
      },
      "source": [
        "One of the shortcomings of CNN is that it is difficult to interpret the results from the neural network. One easy way to assess how the CNN model used spectral variables for the prediction of soil properties via sensitivity analysis. \n",
        "\n",
        "First, a new data frame was created by averaging all the calibration spectra data, and duplicated to the same number of samples as test data. The first five wavelengths of these new averaged spectra were replaced with the actual value from the test data. This simulated spectral data frame was fed into the trained CNN model, and the predictions of soil properties were given. The variance for the prediction of the four properties was then calculated as a measure of the importance of the spectral variables. This process was repeated until all the wavelengths had been evaluated as only five wavelengths were observed at any time.\n",
        "\n",
        "If the wavelengths that were varied are important in predicting that particular properties, we expected the variance to be higher. Conversely, if the wavelengths that were changed were not important, then there should be low variance because the rest of the wavelengths’ variables are the averages of the test dataset. The sensitivity of spectral variables was unique for each soil property and could be related to the band assignments for particular wavelengths. Because the CNN model contained several shared layers, some of these selected spectral variables overlapped with different level of importance. "
      ],
      "id": "9ff7347b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:13.191004Z",
          "start_time": "2021-11-05T04:12:13.176016Z"
        },
        "id": "55e1972a"
      },
      "source": [
        "import random\n",
        "\n",
        "### trying to find important variables used in prediction\n",
        "nvar = 5 #change 5 variables/wavelength at a time\n",
        "ntest = X_cal_MIR.shape[1]-nvar+1 # calculate number of tests\n",
        "\n",
        "# average the calibration spectra\n",
        "avemat = X_cal_MIR.mean(axis=0)\n",
        "avemat = pd.DataFrame(avemat).T\n",
        "\n",
        "# duplicate the average spectra to match the number of test data\n",
        "avemat = avemat.append([avemat]*(len(X_test_MIR)-1),ignore_index=True)"
      ],
      "id": "55e1972a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:47.068284Z",
          "start_time": "2021-11-05T04:12:13.191974Z"
        },
        "id": "5c629ac9",
        "scrolled": true
      },
      "source": [
        "# create empty matrix to save the important variables\n",
        "VarImp = np.zeros(shape=(ntest,y_cal.shape[1]))\n",
        "\n",
        "for xx in range(0,ntest):\n",
        "    \n",
        "#     create a dummy matrix to get the average spectra matrix calculated previously\n",
        "    dummymat = avemat\n",
        "    \n",
        "#     replace the dummy matrix with actual values from the test set\n",
        "    dummymat.iloc[:,xx:xx+nvar] = X_test_MIR[:,xx:xx+nvar]\n",
        "\n",
        "    ## reshape the dummy matrix into 3D matrix\n",
        "    tempmat = np.expand_dims(dummymat,axis=2)\n",
        "    \n",
        "#     create the predictions\n",
        "    TT = model_mult.predict(tempmat)\n",
        "    Y_pred_new = np.hstack(TT)\n",
        "    Y_pred_newT = sc_y.inverse_transform(Y_pred_new)\n",
        "    \n",
        "#    determine the variance of the predictions\n",
        "    tmp = Y_pred_newT.var(axis=0)\n",
        "    \n",
        "#     save the results\n",
        "    VarImp[xx] = tmp"
      ],
      "id": "5c629ac9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:47.084242Z",
          "start_time": "2021-11-05T04:12:47.069281Z"
        },
        "id": "bf8f306f"
      },
      "source": [
        "VarImp = pd.DataFrame(VarImp.T)\n",
        "\n",
        "wave_imp = wavet[2:-2]\n",
        "# rename the columns using the wavelengths\n",
        "VarImp.columns = wave_imp"
      ],
      "id": "bf8f306f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:47.274299Z",
          "start_time": "2021-11-05T04:12:47.085240Z"
        },
        "id": "7d18b3e3"
      },
      "source": [
        "#Let's plot to observe which wavelengths were utilized within the model for the prediction of various properties\n",
        "for i in range(VarImp.shape[0]):\n",
        "    plt.rcParams['figure.figsize'] = [10,12]\n",
        "    plt.subplot(4,1,i+1)\n",
        "    plt.plot(wave_imp, VarImp.iloc[i,:].T)\n",
        "    plt.title(properties[i], x=0.95, y= .8, fontsize ='small', loc='left')\n",
        "    \n",
        "    ax = plt.gca()\n",
        "    ax.invert_xaxis()\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "id": "7d18b3e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-05T04:12:47.290256Z",
          "start_time": "2021-11-05T04:12:47.275297Z"
        },
        "id": "Z_nWAD9_cz1c"
      },
      "source": [
        "end = time.time()\n",
        "print('Time elapsed:', end - start)"
      ],
      "id": "Z_nWAD9_cz1c",
      "execution_count": null,
      "outputs": []
    }
  ]
}